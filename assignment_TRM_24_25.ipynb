{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3Apf-eCqgtl"
      },
      "source": [
        "# Document Analysis and Summarization System\n",
        "\n",
        "##Deadline\n",
        "**Friday January 24 by 23:59 at the latest**. Please do not submit your assignment after the deadline as late submissions will not be graded.\n",
        "\n",
        "## Learning Objectives:\n",
        "\n",
        "* Work with text data in Python\n",
        "* Understand basic text preprocessing\n",
        "* Use simple APIs for text analysis\n",
        "* Collaborate on a coding project\n",
        "* Create a basic command-line interface\n",
        "\n",
        "## Project Description:\n",
        "Your team of 4 will build a Python program that helps analyze and summarize documents. The program should:\n",
        "\n",
        "** Session 1 (~ 3 hours):\n",
        "\n",
        "\n",
        "* Read and preprocess text files\n",
        "* Calculate basic text statistics (word count, sentence count, average word length)\n",
        "* Find most common words and phrases\n",
        "* Generate and show 3 word clouds\n",
        "\n",
        "\n",
        "** Session 2 (~ 3 hours):\n",
        "\n",
        "\n",
        "* Use the Hugging Face Transformers library (https://huggingface.co/docs/hub/en/transformers) to: Generate summaries of the news articles.\n",
        "\n",
        "* Create a simple command-line interface to run all analyses\n",
        "* Save dataframe into a CSV file\n",
        "\n",
        "**Please note that I suggest the time that the assignment might take you. This is a mere guide and does not mean that is all the time you have. Take the time that you need**\n",
        "\n",
        "## Use the following News Articles Dataset:\n",
        "\n",
        "BBC News Dataset: https://www.kaggle.com/datasets/hgultekin/bbcnewsarchive\n",
        "Contains ~2000 news articles in 5 categories. For your task, please use the column '**Content**' of this dataset. Use a sample of 500 news articles. Make sure your sample contains articles from all 5 categories.\n",
        "\n",
        "## Deliverable\n",
        "One self contained fully functional Notebook. Please send only the Notebook as your submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jBJeCxIaq7TK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\rosam\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\rosam\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\rosam\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: Import required libraries\n",
        "# Hint: You'll need nltk, pandas, matplotlib, wordcloud, and transformers\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "import wordcloud\n",
        "\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsPU8N11yOyO"
      },
      "outputs": [],
      "source": [
        "class DocumentAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the DocumentAnalyzer with necessary resources\"\"\"\n",
        "        # TODO: Initialize stop words and the summarization pipeline\n",
        "        self.stop_words = set(stopwords.words('english'))  # Initialize stopwords\n",
        "        self.summarizer = None  # Initialize the summarization pipeline\n",
        "\n",
        "    def process_dataframe(self, df, text_column):\n",
        "        \"\"\"Process text data from a pandas DataFrame column\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input DataFrame\n",
        "            text_column (str): Name of the column containing text data\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with added analysis columns\n",
        "        \"\"\"\n",
        "        # TODO: Implement DataFrame processing\n",
        "        # 1. Read the 500 BBC news articles into the DataFrame. According to your selection criteria.\n",
        "        # 2. Apply text analysis functions to the 'content' column\n",
        "        # 3. Add and populate a new column for each of the following results: number of sentences (num_sentences), number of words (num_words),\n",
        "        #    average word length (avg_word_length), average sentence length (avg_sentence_length), top 10 most common words (common_words),\n",
        "        #    top 5 most common phrases (of lenght 2, i.e., two words),\n",
        "        #    Note: you have to make these calculations for each of the 500 news articles. Only use the 'content' column.\n",
        "        # 4. Handle errors appropriately\n",
        "        df = df.sample(500)\n",
        "        \n",
        "        num_sentences, num_words, avg_word_length, avg_sentence_length = df.apply(basic_stats())\n",
        "\n",
        "    def basic_stats(self, text):\n",
        "        \"\"\"Calculate basic text statistics\"\"\"\n",
        "        # TODO: Calculate and return a dictionary containing:\n",
        "        # - Number of sentences (num_sentences)\n",
        "        # - Number of words (num_words)\n",
        "        # - Average word length (avg_word_length)\n",
        "        # - Average sentence length (avg_sentence_length)\n",
        "        pass\n",
        "\n",
        "    def get_common_words(self, text, n=10):\n",
        "        \"\"\"Find the n most common words in the text\"\"\"\n",
        "        # TODO: Implement word frequency analysis\n",
        "        # Remember to:\n",
        "        # 1. Tokenize the text\n",
        "        # 2. Convert to lowercase\n",
        "        # 3. Remove stopwords\n",
        "        # 4. Count frequencies\n",
        "        pass\n",
        "\n",
        "    def get_common_phrases(self, text, n=5, phrase_length=2):\n",
        "        \"\"\"Find the n most common phrases of specified length\"\"\"\n",
        "        # TODO: Implement phrase frequency analysis using nltk.util.ngrams\n",
        "        pass\n",
        "\n",
        "    def create_wordcloud(self, text):\n",
        "        \"\"\"Generate and save a word cloud visualization\"\"\"\n",
        "        # TODO: Create and show word cloud visualizations of the most common words for 3 randomly selected news articles.\n",
        "        # Use WordCloud class and matplotlib\n",
        "        pass\n",
        "\n",
        "    def generate_summary(self, text, max_length=50, min_length=30):\n",
        "        \"\"\"Generate a summary using the BART model\"\"\"\n",
        "        # TODO: Implement text summarization\n",
        "        # Remember to:\n",
        "        # 1. Handle long texts by splitting into chunks\n",
        "        # 2. Use the summarization pipeline\n",
        "        # 3. Combine summaries if needed\n",
        "        pass\n",
        "\n",
        "    def _split_into_chunks(self, text, max_chunk_size=1000):\n",
        "        \"\"\"Helper method to split text into chunks for summarization\"\"\"\n",
        "        # TODO: Implement text splitting into chunks\n",
        "        # This is needed because the summarizer has a maximum input length\n",
        "        pass\n",
        "\n",
        "    def save_analysis(self, filepath, analysis_results):\n",
        "        \"\"\"Save analysis results to a CSV file\"\"\"\n",
        "        # TODO: Implement saving dataframe to a CSV file\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDzbpPzeyYct"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # TODO: Implement the main function that:\n",
        "    # 1. Initializes the analyzer\n",
        "    # 2. Populates the DataFrame with 500 BBC news articles\n",
        "    # 3. Processes the DataFrame\n",
        "    # 4. Generates worldclouds for 5 random summaries\n",
        "    # 5. Saves the resulting updated dataframe to CSV\n",
        "\n",
        "    # Hint: Start by loading your 500 news articles into a dataframe. Make sure to include the code to select those 500 news articles.\n",
        "    filepath = 'bbc-news-data.csv' #path_to_data_file - Replace with your text file\n",
        "    df = pd.DataFrame(pd.read_csv(filepath, sep='\t', engine='python'))\n",
        "    # TODO: Complete the implementation\n",
        "    pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Testing functions\n",
        "def run_tests():\n",
        "    # TODO: Implement tests for your functions\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
